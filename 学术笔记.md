# 论文笔记
## 一、EEG综述类

## 二、EEG无代码论文

## 三、EEG有代码论文

## 四、计算机视觉有代码论文

## 五、计算机视觉无代码论文
## 六、复现论文笔记
### Neuro-3d
第二步运行替换为
```
python recon_main.py --data_path "D:/project/neuro-3d/EEGdata/" --generation_type shape --sub sub01 --in_channels 1027 --pretrain_model "retri_color_shape_12-23_16-56_VideoImageEEGClassifyColor3_color_video_fea_time_len1" --num_workers 0
```


# 小技巧随记

### 1.GCN知识

标准图卷积网络（GCN）层（Kipf & Welling 2017）简化卷积

GCN 层通过对邻接矩阵的归一化，将消息聚合和特征转换合并在一个线性操作中。GCN 层的特征传播公式： $\mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right)$ ，其中 **$H^{(l)} \in \mathbb{R}^{N \times F_{in}}$** ：第 $l$ 层的节点特征矩阵。$N$ 是节点数， $F_{in}$  是输入特征维度，** $W^{(l)} \in \mathbb{R}^{F_{in} \times F_{out}}$ **：第  $l$  层可学习的权重矩阵（线性变换），** $\tilde{A} = A + I_N$ **：添加了自环（Self-loops）的邻接矩阵。为什么要加  $I_N$ ？如果不加自环，节点在更新时只聚合邻居的信息，而忽略了自身的特征。 $\tilde{A}$ 确保了自身特征也参与下一次迭代， $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$**：   $\tilde{A}$ 的度矩阵（Degree Matrix），是一个对角矩阵， $\sigma(\cdot)$ **：非线性激活函数，通常使用  $\text{ReLU}$ 。

前向传播：
**第一步**：特征变换 首先对节点特征进行线性投影：$Z^{(l)} = H^{(l)} W^{(l)}$维度变换$(N \times F_{in}) \times (F_{in} \times F_{out}) \rightarrow (N \times F_{out})$，物理意义：这一步类似于传统 CNN 中的 $1 \times 1$ 卷积或 MLP，旨在将特征映射到新的高维或低维空间，以提取更高阶的语义特征。 
**第二步**：图结构归一化与消息聚合这是GCN的灵魂所在。我们将变换后的特征 $Z^{(l)}$ 左乘归一化的邻接矩阵：$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ ,$H_{agg} = \hat{A} Z^{(l)}$ , 对称归一化：$\hat{A}_{ij} = \frac{\tilde{A}_{ij}}{\sqrt{\tilde{d}_i \tilde{d}_j}}$。相比于随机游走归一化（$D^{-1}A$），对称归一化保持了矩阵的对称性，这对谱分析至关重要。物理意义：它在聚合邻居信息时，不仅考虑了节点 $i$ 自身的度（度大则权重低，避免hub节点过度支配），也考虑了邻居 $j$ 的度（邻居度大，说明该邻居的影响力被分散了，传给 $i$ 的权重也应降低）。消息传递：这一步实现了局部平滑，每个节点 $i$ 收集其一阶邻居（包括自己）的加权特征和。

第一步：图卷积的原始定义（谱域视角）
​	在图像中，卷积是利用固定大小的核在网格上滑动。但在图上，节点排列无序，无法直接“滑动”。 于是数学家利用**卷积定理**：$$f * g = \mathcal{F}^{-1} \big( \mathcal{F}(f) \cdot \mathcal{F}(g) \big)$$即：时域（空域）的卷积等于频域的乘积。要在图上做卷积，我们需要定义图的“傅里叶变换”。基底：图的拉普拉斯矩阵 $L = D - A$ 的特征向量矩阵 $U$ 就是图的傅里叶基。变换：图信号 $x$ 的傅里叶变换是 $\hat{x} = U^T x$。那么，**图卷积最初被定义为**：$$g_{\theta} \star x = U g_{\theta}(\Lambda) U^T x$$，其中：$x$ 是输入信号（节点特征）。$U$ 是拉普拉斯矩阵的特征向量。$g_{\theta}(\Lambda)$ 是卷积核（滤波器），是我们需要学习的对角矩阵参数。**痛点**：计算特征向量矩阵 $U$ 需要对 $L$ 进行特征分解，复杂度是 $O(N^3)$。对于大图，这根本不可行。
第二步：切比雪夫多项式近似
​	为了避免计算 $U$，学者们引入了**切比雪夫多项式**来近似那个卷积核 $g_{\theta}(\Lambda)$。原理是：任何函数都可以用多项式级数逼近。此时卷积变成了：$$g_{\theta'} \star x \approx \sum_{k=0}^K \theta'_k T_k(\tilde{L}) x$$​，$T_k$ 是 $k$ 阶切比雪夫多项式，$\tilde{L}$ 是归一化后的拉普拉斯矩阵。**K**：代表卷积核的大小，也就是感受野。$K=1$ 代表只看一阶邻居，$K=2$ 代表看二阶邻居。**好处**：不再需要特征分解，只需要算矩阵的幂 $L^k$，这就变成了局部操作，计算量大大降低。
第三步：Kipf & Welling 的“一阶近似” (GCN 2017)
​	Kipf 做了一个非常大胆的简化，把上面的切比雪夫近似推到了极致。1. 限制 K=1（一阶近似）Kipf 认为，我们不需要在单层里看得很远，只看一阶邻居（$K=1$）就够了，通过堆叠多层网络来扩大感受野。当 $K=1$ 时，公式简化为关于 $\tilde{L}$ 的线性函数：$$g_{\theta'} \star x \approx \theta'_0 x + \theta'_1 \tilde{L} x$$，参数共享与约束现在的公式里有两个参数 $\theta'_0$ 和 $\theta'_1$。为了防止过拟合和减少计算量，Kipf 强制令 $\theta'_0 = - \theta'_1 = \theta$。结合拉普拉斯矩阵的定义（$\tilde{L} = I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$），推导过程如下：$$\begin{aligned} g \star x &\approx \theta(I) x - \theta(\tilde{L}) x \\ &= \theta(I - \tilde{L}) x \\ &= \theta \left( I - (I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}) \right) x \\ &= \theta \left( \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} \right) x \end{aligned}$$

看！这就出现了 $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 这个核心结构。Renormalization Trick (重归一化技巧)上面推导出的 $\tilde{A}$ 是原始邻接矩阵，对角线为0。如果不处理，直接乘这个矩阵，经过深层网络后，特征值的范围会使得数值不稳定（梯度爆炸或消失）。于是，Kipf 提出了 Renormalization Trick：直接用 $\tilde{A} = A + I$ （加了自环的邻接矩阵）来代替上面的结构。最终公式就诞生了：$$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} X W$$
![image-20251203101955952|500](assets/学术笔记/image-20251203101955952.png)
### 3.模块缝合问题
```python
    def forward(self, x):
        x1 = self.in_conv(x)
        x2 = self.down1(x1)
        x2 = self.lsk(x2)
        x3 = self.down2(x2)
        print("x3",x3.shape)
        x3 = to_3d(x3)
        print("x3",x3.shape)
        x3 = self.mv(x3)
        x3 = to_4d(x3,16,16)#这里🔎to_3d要加入两个参数  to_4d(x3,16,16)后面两个16是原始数据的h和w
        # x = x.permute(0, 2, 3, 1)  # 【B, C, H, W】 -> 【B, H, W, C】🔎这个地方尺寸问题 很重要
        # x= x.permute(0, 3, 1, 2)  # 【B, H, W, C】 -> 【B, C, H, W】
        x3 = x3.permute(0, 2, 3, 1)
        x3 = self.md(x3)
        x3 = x3.permute(0, 3, 1, 2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        x = self.out_conv(x)
        return x
# 输入 B C H W,  输出 B C H W
if __name__ == '__main__':
    block = UNet()
    input = torch.rand(3, 1, 64, 64)
    output = block(input)
    print(input.size(), output.size())

```

记得print("符号：",x.shape)，接下来就是bhwc这种尺寸问题：batch：0（有的时候是N），channel：1，height：2，width：3   👉(B/N,C,H,W)

### 4.读文献
通过 “为何开展研究（Why）— 研究得出何结论（What）— 研究如何实施（How）” 的 WWH 分析框架，挖掘现有文献的局限性，进而提炼可突破的科研创新点，即实现**WWH→科研 Ideas**的转化。
### 5.创新点
1.**文献缺陷识别**科研创新需依托顶刊文献研判，核心是定位其不足：XX 问题研究空白；XX 问题 / 工况缺有效分析方法；仅述试验现象未揭物理机制、建预测模型；结论无实测验证；理论假设欠合理、推导不严谨；数值计算参数冗余难标定、效率低且误差大；试验数据不足以支撑结论。
2.**创新优先级排序**按学术贡献与价值从高到低为：**新问题 - 新方法**（顶刊级、学科引领性）＞**老问题 - 新方法**（顶 / 主流期刊级、技术革新性）＞**新问题 - 老方法**（主流期刊级、方法拓展性）＞**老问题 - 老方法**（创新价值最低）。
### 6.打印模型细节

```python
if __name__=='__main__':
	vit = VisionTransformer()
	print(vit)
```
### 7.python基础
```python
# 定义一个类（蓝图）
class Dog:
    # __init__ 是构造函数，在实例化时自动调用 __强制私有
    # self 代表实例本身
    def __init__(self, name, breed):
        # 给实例添加属性
        self.name = name
        self.breed = breed

    # 给实例添加方法
    def bark(self):
        print(f"{self.name} 汪汪叫！")

# --- 实例化过程 ---
# 根据 Dog 类，创建一个名为 "旺财" 的实例
my_dog = Dog("旺财", "中华田园犬")

# my_dog 就是一个实例，它拥有 name 和 breed 属性，以及 bark 方法
print(my_dog.name)  # 输出: 旺财
print(my_dog.breed) # 输出: 中华田园犬
my_dog.bark()       # 输出: 旺财 汪汪叫！
```

### 8.BrainNet Viewer 10-20通道 node文件

-21.5	66.9	12.1	1	1	Fp1
24.3	66.3	12.5	1	1	Fp2
-39.7	25.3	44.7	2	1	F3
41.9	27.5	43.9	2	1	F4
-49.1	-20.7	53.2	3	1	C3
50.3	-18.8	53.0	3	1	C4
-41.4	-67.8	42.4	5	1	P3
44.2	-65.8	42.7	5	1	P4
-25.8	-93.3	7.7	6	1	O1
25.0	-95.2	6.2	6	1	O2
-52.1	28.6	3.8	2	1	F7
53.2	28.4	-3.1	2	1	F8
-65.8	-17.8	-2.9	4	1	T7
67.4	-18.5	-3.4	4	1	T8
-55.9	-64.8	0.0	4	1	P7
56.4	-64.4	0.1	4	1	P8
0.0	26.8	60.6	2	1	Fz
0.8	-21.9	77.4	3	1	Cz
0.7	-69.3	56.9	5	1	Pz
-62.8	-46.6	-14.66	4	1	TP9
62.8	-46.58	-14.6	4	1	TP10  

### 9.Diffusion知识

![image-20251126151607551|525](assets/学术笔记/image-20251126151607551.png)

$q(x_t|x_{t-1})$ 这个表明他是在前一项的基础上加噪声所以是条件概率（马尔可夫性我们只需要知道它紧邻的上一步 $x_{t-1}$ 就足够了。**我们不需要知道 $x_0, x_1, \dots, x_{t-2}$ 是什么样子的**），选择高斯噪声原因是两个独立高斯分布随机变量相加，结果仍然服从高斯分布，

<img src="assets/学术笔记/image-20251127192513401.png" alt="image-20251127192513401" style="zoom: 67%;" />$z \sim \mathcal{N}(\mu, \sigma^2)$代表定义了一个随机变量z，它服从一个一般正态分布（高斯分布)。这个分布均值（中心位置是$\mu$)，方差（离散程度是$\sigma^2$)，$\frac{z - \mu}{\sigma} \sim \mathcal{N}(0, I)$这是一个标准化过程，如果我们把变量 $z$ 减去它的均值 $\mu$，再除以它的标准差 $\sigma$，得到的新变量就会服从标准正态分布。标准正态分布是最简单的高斯分布，其均值为 0，方差为 1（或者在**多维情况下为单位矩阵 $I$**)**这种变换后的变量常被称为 Z-score。***$z = \mu + \sigma \cdot \epsilon$*    和 *$\epsilon \sim \mathcal{N}(0, I)$* 这两行是逆操作，也是重参数化技巧核心，引入服从标准正态分布的辅助噪声变量$\epsilon$
任意一个复杂的正态分布变量 $z$，都可以看作是由一个确定的均值部分 $\mu$，加上一个被标准差 $\sigma$ 缩放过的标准噪声 $\epsilon$ 组合而成的。
**$x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_{t-1}$** 这里的均值 $\mu$ 是 $\sqrt{\alpha_t}x_{t-1}$，这里的方差 $\sigma^2$ 是 $1-\alpha_t$，所以标准差 $\sigma$ 是 $\sqrt{1-\alpha_t}$

### 10.深度学习路线

李沐--吴恩达---李宏毅--爆肝杰哥（入门）--邱锡鹏--小土堆（入门）--shuhuai008（硬核手推）--

### 11.知识蒸馏

![img|425](https://i0.hdslb.com/bfs/note/6bb4f522afcadd0aee9d2a51abfbbfb6fb0fb7b7.png@980w_!web-note.webp)

T为1，就是原softmax函数，softmax本来就是把每个类别的logic强行变成0-1之间的概率，并且求和为1，是有放大差异的功能，如果logic高一点点，经过softmax，都会变的很高。T越小，非正确类别的概率相对大小的信息就会暴露的更明显；T越大，曲线就会变得更soft，高的概率给降低，低的概率会变高，贫富差距就没有了。
**举个例子：**
**左侧**学生网络是个神经网络，四个类别有一个线性分类层，猫的类别给出的分数logit为-5，狗的类别给出的分数logit为2，驴的类别给出的分数logit为7，马的类别给出的分数logit为9原来的softmax在图中是当T=1时的计算，从计算结果可以看出，有不同数量级，贫富差异较大；当T=3时，得到的分布就更软了，基本是同一个数量级。但是还是几个数值均分布在0-1之间且和为1
**右侧**教师网络也是

![img|875](https://i0.hdslb.com/bfs/note/76a03e3548f62fa34d557a82ffe5eddaa10e1da4.png@980w_!web-note.webp)

知识蒸馏的过程

![img|450](https://i0.hdslb.com/bfs/note/010cff3905046e775abe9dfee2b76d33a24df6a0.png@960w_!web-note.webp)


### 12.准确率陷阱

![image-20251202221938438|450](assets/学术笔记/image-20251202221938438.png)

注意力陷阱模型只学习一个样本类别，放弃另一个样本类别

**本质**：你的模型**根本没有学习**。它找到了一个“偷懒”的全局最优解——**把所有样本都预测为 Class 0（负样本/Normal）**。

**数据推断**：根据 Class 0 R: 0.967 和 Class 1 R: 0.000，可以反推你的数据集中，**Class 0 占比约为 97%，而 Class 1（MCI）占比仅为 3% 左右**。

### 13.kl散度可视化操作（ipywidgets）

KL 散度 (Forward KL) 的连续形式定义如下：

$$D_{KL}(P || Q) = \int_{-\infty}^{\infty} \underbrace{P(x)}_{\text{权重}} \cdot \underbrace{\log \left( \frac{P(x)}{Q(x)} \right)}_{\text{对数比 (Log-Ratio)}} dx$$

为了与图像对应，我们将被积函数定义为 **“局部 KL 贡献密度” (Local Contribution)**，记为 $f(x)$：

$$f(x) = P(x) \cdot [\log P(x) - \log Q(x)]$$

您在下方子图中看到的红色和绿色曲线，画的就是 $y = f(x)$。
区域颜色积分符号概率关系数学本质 (Log Ratio)物理直觉
红色 (Cost)**$(+)$ 正值$P(x) > Q(x)$$\log(P/Q) > 0$**欠拟合 (Under-estimation)**老师讲了重点，学生没记住。
**绿色 (Benefit)**$(-)$ 负值$P(x) < Q(x)$$\log(P/Q) < 0$**过拟合 (Over-estimation)**老师没讲的废话，学生背了一堆。

<img src="assets/学术笔记/image-20251204150952347.png" alt="image-20251204150952347" style="zoom:50%;" />

### 13.ipywidgets
要在shell先启动环境在激活notebook，输出图像时候要记得定高。

<img src="assets/学术笔记/gcn-dned2v.gif" alt="gcn-dned2v" style="zoom:50%;" />
# 论文笔记
## 一、EEG综述类

## 二、EEG无代码论文

## 三、EEG有代码论文

## 四、计算机视觉有代码论文

## 五、计算机视觉无代码论文

# 零碎知识点随记

### 1.GCN知识

标准图卷积网络（GCN）层（Kipf & Welling 2017）简化卷积

GCN 层通过对邻接矩阵的归一化，将消息聚合和特征转换合并在一个线性操作中。GCN 层的特征传播公式： $\mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right)$ ，其中 **$H^{(l)} \in \mathbb{R}^{N \times F_{in}}$** ：第 $l$ 层的节点特征矩阵。$N$ 是节点数， $F_{in}$  是输入特征维度，** $W^{(l)} \in \mathbb{R}^{F_{in} \times F_{out}}$ **：第  $l$  层可学习的权重矩阵（线性变换），** $\tilde{A} = A + I_N$ **：添加了自环（Self-loops）的邻接矩阵。为什么要加  $I_N$ ？如果不加自环，节点在更新时只聚合邻居的信息，而忽略了自身的特征。 $\tilde{A}$ 确保了自身特征也参与下一次迭代， $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$**：   $\tilde{A}$ 的度矩阵（Degree Matrix），是一个对角矩阵， $\sigma(\cdot)$ **：非线性激活函数，通常使用  $\text{ReLU}$ 。

前向传播：
**第一步**：特征变换 首先对节点特征进行线性投影：$Z^{(l)} = H^{(l)} W^{(l)}$维度变换$(N \times F_{in}) \times (F_{in} \times F_{out}) \rightarrow (N \times F_{out})$，物理意义：这一步类似于传统 CNN 中的 $1 \times 1$ 卷积或 MLP，旨在将特征映射到新的高维或低维空间，以提取更高阶的语义特征。 
**第二步**：图结构归一化与消息聚合这是GCN的灵魂所在。我们将变换后的特征 $Z^{(l)}$ 左乘归一化的邻接矩阵：$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ ,$H_{agg} = \hat{A} Z^{(l)}$ , 对称归一化：$\hat{A}_{ij} = \frac{\tilde{A}_{ij}}{\sqrt{\tilde{d}_i \tilde{d}_j}}$。相比于随机游走归一化（$D^{-1}A$），对称归一化保持了矩阵的对称性，这对谱分析至关重要。物理意义：它在聚合邻居信息时，不仅考虑了节点 $i$ 自身的度（度大则权重低，避免hub节点过度支配），也考虑了邻居 $j$ 的度（邻居度大，说明该邻居的影响力被分散了，传给 $i$ 的权重也应降低）。消息传递：这一步实现了局部平滑，每个节点 $i$ 收集其一阶邻居（包括自己）的加权特征和。

第一步：图卷积的原始定义（谱域视角）
​	在图像中，卷积是利用固定大小的核在网格上滑动。但在图上，节点排列无序，无法直接“滑动”。 于是数学家利用**卷积定理**：$$f * g = \mathcal{F}^{-1} \big( \mathcal{F}(f) \cdot \mathcal{F}(g) \big)$$即：时域（空域）的卷积等于频域的乘积。要在图上做卷积，我们需要定义图的“傅里叶变换”。基底：图的拉普拉斯矩阵 $L = D - A$ 的特征向量矩阵 $U$ 就是图的傅里叶基。变换：图信号 $x$ 的傅里叶变换是 $\hat{x} = U^T x$。那么，**图卷积最初被定义为**：$$g_{\theta} \star x = U g_{\theta}(\Lambda) U^T x$$，其中：$x$ 是输入信号（节点特征）。$U$ 是拉普拉斯矩阵的特征向量。$g_{\theta}(\Lambda)$ 是卷积核（滤波器），是我们需要学习的对角矩阵参数。**痛点**：计算特征向量矩阵 $U$ 需要对 $L$ 进行特征分解，复杂度是 $O(N^3)$。对于大图，这根本不可行。
第二步：切比雪夫多项式近似
​	为了避免计算 $U$，学者们引入了**切比雪夫多项式**来近似那个卷积核 $g_{\theta}(\Lambda)$。原理是：任何函数都可以用多项式级数逼近。此时卷积变成了：$$g_{\theta'} \star x \approx \sum_{k=0}^K \theta'_k T_k(\tilde{L}) x$$​，$T_k$ 是 $k$ 阶切比雪夫多项式，$\tilde{L}$ 是归一化后的拉普拉斯矩阵。**K**：代表卷积核的大小，也就是感受野。$K=1$ 代表只看一阶邻居，$K=2$ 代表看二阶邻居。**好处**：不再需要特征分解，只需要算矩阵的幂 $L^k$，这就变成了局部操作，计算量大大降低。
第三步：Kipf & Welling 的“一阶近似” (GCN 2017)
​	Kipf 做了一个非常大胆的简化，把上面的切比雪夫近似推到了极致。1. 限制 K=1（一阶近似）Kipf 认为，我们不需要在单层里看得很远，只看一阶邻居（$K=1$）就够了，通过堆叠多层网络来扩大感受野。当 $K=1$ 时，公式简化为关于 $\tilde{L}$ 的线性函数：$$g_{\theta'} \star x \approx \theta'_0 x + \theta'_1 \tilde{L} x$$，参数共享与约束现在的公式里有两个参数 $\theta'_0$ 和 $\theta'_1$。为了防止过拟合和减少计算量，Kipf 强制令 $\theta'_0 = - \theta'_1 = \theta$。结合拉普拉斯矩阵的定义（$\tilde{L} = I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$），推导过程如下：$$\begin{aligned} g \star x &\approx \theta(I) x - \theta(\tilde{L}) x \\ &= \theta(I - \tilde{L}) x \\ &= \theta \left( I - (I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}) \right) x \\ &= \theta \left( \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} \right) x \end{aligned}$$

看！这就出现了 $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 这个核心结构。Renormalization Trick (重归一化技巧)上面推导出的 $\tilde{A}$ 是原始邻接矩阵，对角线为0。如果不处理，直接乘这个矩阵，经过深层网络后，特征值的范围会使得数值不稳定（梯度爆炸或消失）。于是，Kipf 提出了 Renormalization Trick：直接用 $\tilde{A} = A + I$ （加了自环的邻接矩阵）来代替上面的结构。最终公式就诞生了：$$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} X W$$
![image-20251203101955952|500](assets/学术笔记/image-20251203101955952.png)
### 3.模块缝合问题
```python
    def forward(self, x):
        x1 = self.in_conv(x)
        x2 = self.down1(x1)
        x2 = self.lsk(x2)
        x3 = self.down2(x2)
        print("x3",x3.shape)
        x3 = to_3d(x3)
        print("x3",x3.shape)
        x3 = self.mv(x3)
        x3 = to_4d(x3,16,16)#这里🔎to_3d要加入两个参数  to_4d(x3,16,16)后面两个16是原始数据的h和w
        # x = x.permute(0, 2, 3, 1)  # 【B, C, H, W】 -> 【B, H, W, C】🔎这个地方尺寸问题 很重要
        # x= x.permute(0, 3, 1, 2)  # 【B, H, W, C】 -> 【B, C, H, W】
        x3 = x3.permute(0, 2, 3, 1)
        x3 = self.md(x3)
        x3 = x3.permute(0, 3, 1, 2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        x = self.out_conv(x)
        return x
# 输入 B C H W,  输出 B C H W
if __name__ == '__main__':
    block = UNet()
    input = torch.rand(3, 1, 64, 64)
    output = block(input)
    print(input.size(), output.size())

```

记得print("符号：",x.shape)，接下来就是bhwc这种尺寸问题：batch：0（有的时候是N），channel：1，height：2，width：3   👉(B/N,C,H,W)

### 4.读文献
通过 “为何开展研究（Why）— 研究得出何结论（What）— 研究如何实施（How）” 的 WWH 分析框架，挖掘现有文献的局限性，进而提炼可突破的科研创新点，即实现**WWH→科研 Ideas**的转化。
### 5.创新点
1.**文献缺陷识别**科研创新需依托顶刊文献研判，核心是定位其不足：XX 问题研究空白；XX 问题 / 工况缺有效分析方法；仅述试验现象未揭物理机制、建预测模型；结论无实测验证；理论假设欠合理、推导不严谨；数值计算参数冗余难标定、效率低且误差大；试验数据不足以支撑结论。
2.**创新优先级排序**按学术贡献与价值从高到低为：**新问题 - 新方法**（顶刊级、学科引领性）＞**老问题 - 新方法**（顶 / 主流期刊级、技术革新性）＞**新问题 - 老方法**（主流期刊级、方法拓展性）＞**老问题 - 老方法**（创新价值最低）。
### 6.打印模型细节

```python
if __name__=='__main__':
	vit = VisionTransformer()
	print(vit)
```
### 7.python基础
```python
# 定义一个类（蓝图）
class Dog:
    # __init__ 是构造函数，在实例化时自动调用 __强制私有
    # self 代表实例本身
    def __init__(self, name, breed):
        # 给实例添加属性
        self.name = name
        self.breed = breed

    # 给实例添加方法
    def bark(self):
        print(f"{self.name} 汪汪叫！")

# --- 实例化过程 ---
# 根据 Dog 类，创建一个名为 "旺财" 的实例
my_dog = Dog("旺财", "中华田园犬")

# my_dog 就是一个实例，它拥有 name 和 breed 属性，以及 bark 方法
print(my_dog.name)  # 输出: 旺财
print(my_dog.breed) # 输出: 中华田园犬
my_dog.bark()       # 输出: 旺财 汪汪叫！
```

### 8.BrainNet Viewer 10-20通道 node文件

-21.5	66.9	12.1	1	1	Fp1
24.3	66.3	12.5	1	1	Fp2
-39.7	25.3	44.7	2	1	F3
41.9	27.5	43.9	2	1	F4
-49.1	-20.7	53.2	3	1	C3
50.3	-18.8	53.0	3	1	C4
-41.4	-67.8	42.4	5	1	P3
44.2	-65.8	42.7	5	1	P4
-25.8	-93.3	7.7	6	1	O1
25.0	-95.2	6.2	6	1	O2
-52.1	28.6	3.8	2	1	F7
53.2	28.4	-3.1	2	1	F8
-65.8	-17.8	-2.9	4	1	T7
67.4	-18.5	-3.4	4	1	T8
-55.9	-64.8	0.0	4	1	P7
56.4	-64.4	0.1	4	1	P8
0.0	26.8	60.6	2	1	Fz
0.8	-21.9	77.4	3	1	Cz
0.7	-69.3	56.9	5	1	Pz
-62.8	-46.6	-14.66	4	1	TP9
62.8	-46.58	-14.6	4	1	TP10  

### 9.Diffusion知识

![image-20251126151607551|525](assets/学术笔记/image-20251126151607551.png)

$q(x_t|x_{t-1})$ 这个表明他是在前一项的基础上加噪声所以是条件概率（马尔可夫性我们只需要知道它紧邻的上一步 $x_{t-1}$ 就足够了。**我们不需要知道 $x_0, x_1, \dots, x_{t-2}$ 是什么样子的**），选择高斯噪声原因是两个独立高斯分布随机变量相加，结果仍然服从高斯分布，

<img src="assets/学术笔记/image-20251127192513401.png" alt="image-20251127192513401" style="zoom: 67%;" />$z \sim \mathcal{N}(\mu, \sigma^2)$代表定义了一个随机变量z，它服从一个一般正态分布（高斯分布)。这个分布均值（中心位置是$\mu$)，方差（离散程度是$\sigma^2$)，$\frac{z - \mu}{\sigma} \sim \mathcal{N}(0, I)$这是一个标准化过程，如果我们把变量 $z$ 减去它的均值 $\mu$，再除以它的标准差 $\sigma$，得到的新变量就会服从标准正态分布。标准正态分布是最简单的高斯分布，其均值为 0，方差为 1（或者在**多维情况下为单位矩阵 $I$**)**这种变换后的变量常被称为 Z-score。***$z = \mu + \sigma \cdot \epsilon$*    和 *$\epsilon \sim \mathcal{N}(0, I)$* 这两行是逆操作，也是重参数化技巧核心，引入服从标准正态分布的辅助噪声变量$\epsilon$
任意一个复杂的正态分布变量 $z$，都可以看作是由一个确定的均值部分 $\mu$，加上一个被标准差 $\sigma$ 缩放过的标准噪声 $\epsilon$ 组合而成的。
**$x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_{t-1}$** 这里的均值 $\mu$ 是 $\sqrt{\alpha_t}x_{t-1}$，这里的方差 $\sigma^2$ 是 $1-\alpha_t$，所以标准差 $\sigma$ 是 $\sqrt{1-\alpha_t}$

### 10.深度学习路线

李沐--吴恩达---李宏毅--爆肝杰哥（入门）--邱锡鹏--小土堆（入门）--shuhuai008（硬核手推）--

### 11.知识蒸馏

![img|425](https://i0.hdslb.com/bfs/note/6bb4f522afcadd0aee9d2a51abfbbfb6fb0fb7b7.png@980w_!web-note.webp)

T为1，就是原softmax函数，softmax本来就是把每个类别的logic强行变成0-1之间的概率，并且求和为1，是有放大差异的功能，如果logic高一点点，经过softmax，都会变的很高。T越小，非正确类别的概率相对大小的信息就会暴露的更明显；T越大，曲线就会变得更soft，高的概率给降低，低的概率会变高，贫富差距就没有了。
**举个例子：**
**左侧**学生网络是个神经网络，四个类别有一个线性分类层，猫的类别给出的分数logit为-5，狗的类别给出的分数logit为2，驴的类别给出的分数logit为7，马的类别给出的分数logit为9原来的softmax在图中是当T=1时的计算，从计算结果可以看出，有不同数量级，贫富差异较大；当T=3时，得到的分布就更软了，基本是同一个数量级。但是还是几个数值均分布在0-1之间且和为1
**右侧**教师网络也是

![img|875](https://i0.hdslb.com/bfs/note/76a03e3548f62fa34d557a82ffe5eddaa10e1da4.png@980w_!web-note.webp)

知识蒸馏的过程

![img|450](https://i0.hdslb.com/bfs/note/010cff3905046e775abe9dfee2b76d33a24df6a0.png@960w_!web-note.webp)


### 12.准确率陷阱

![image-20251202221938438|450](assets/学术笔记/image-20251202221938438.png)

注意力陷阱模型只学习一个样本类别，放弃另一个样本类别

**本质**：你的模型**根本没有学习**。它找到了一个“偷懒”的全局最优解——**把所有样本都预测为 Class 0（负样本/Normal）**。

**数据推断**：根据 Class 0 R: 0.967 和 Class 1 R: 0.000，可以反推你的数据集中，**Class 0 占比约为 97%，而 Class 1（MCI）占比仅为 3% 左右**。

### 13.kl散度可视化操作（ipywidgets）

KL 散度 (Forward KL) 的连续形式定义如下：

$$D_{KL}(P || Q) = \int_{-\infty}^{\infty} \underbrace{P(x)}_{\text{权重}} \cdot \underbrace{\log \left( \frac{P(x)}{Q(x)} \right)}_{\text{对数比 (Log-Ratio)}} dx$$

为了与图像对应，我们将被积函数定义为 **“局部 KL 贡献密度” (Local Contribution)**，记为 $f(x)$：

$$f(x) = P(x) \cdot [\log P(x) - \log Q(x)]$$

您在下方子图中看到的红色和绿色曲线，画的就是 $y = f(x)$。
区域颜色积分符号概率关系数学本质 (Log Ratio)物理直觉
红色 (Cost)**$(+)$ 正值$P(x) > Q(x)$$\log(P/Q) > 0$**欠拟合 (Under-estimation)**老师讲了重点，学生没记住。
**绿色 (Benefit)**$(-)$ 负值$P(x) < Q(x)$$\log(P/Q) < 0$**过拟合 (Over-estimation)**老师没讲的废话，学生背了一堆。

<img src="assets/学术笔记/image-20251204150952347.png" alt="image-20251204150952347" style="zoom:50%;" />

### 13.ipywidgets
要在shell先启动环境在激活notebook，输出图像时候要记得定高。
<img src="assets/学术笔记/gcn-dned2v.gif" alt="gcn-dned2v" style="zoom:50%;" />

### 14.VAE变分自编码器
 
VAE 是一种通过最小化证据下界（ELBO）的负值来训练的生成模型，其核心思想是强制编码器的后验潜在分布 $q_\phi(z|X)$ 逼近一个简单的**先验分布** $p(z)$，同时最大化**重构数据的似然** $p_\theta(X|z)$，从而实现潜在空间的平滑和可生成性。
***公式提炼*
用公式表示，VAE 最小化的损失函数（负 ELBO）是：
$$\mathcal{L}(\theta, \phi) = \underbrace{-E_{q_\phi(z|X)}[\log p_\theta(X|z)]}_{\text{重构损失 (让 } \hat{X} \approx X \text{)}} + \underbrace{D_{KL}(q_\phi(z|X) || p(z))}_{\text{正则化项 (让 } q \approx p \text{)}}$$
- **重构项：** 确保解码器能准确重构数据。
- **KL 散度项：** 保证潜在向量 $z$ 的分布是平滑且连续的（即 $q_\phi(z|X)$ 接近标准正态先验 $p(z)$）。
### 15 三大可视化 PCA、t-SNE、UMAP
1. 主成分分析 (PCA)，目标： 最大化方差 $\text{Var}(\mathbf{z})$。寻找投影矩阵 $\mathbf{V}_K$。
$\text{Data Centerization: } \mathbf{\tilde{x}}_i = \mathbf{x}_i - \mathbf{\bar{x}}$，$\text{Covariance Matrix: } \mathbf{C} = \frac{1}{N-1} \sum (\mathbf{\tilde{x}}_i)(\mathbf{\tilde{x}}_i)^T$，$\text{Eigen-decomposition: } \mathbf{C} \mathbf{v}_k = \lambda_k \mathbf{v}_k$，$\text{Projection: } \mathbf{z}_i = \mathbf{V}_K^T \mathbf{\tilde{x}}_i$

### 16 交叉熵和KL散度
$H(P, Q) = - \sum_{x} P(x) \log Q(x)$，和，$D_{KL}(P || Q) = \sum_{x} P(x) \log \left( \frac{P(x)}{Q(x)} \right)$
核心等式：$H(P, Q) = D_{KL}(P || Q) + H(P)$
其中$H(P)$ 是信息熵： $H(P) = - \sum_{x} P(x) \log P(x)$。它衡量真实分布 $P$ 本身固有的不确定性或信息量。
$H(P)$是一个常数
### 17 lora低秩矩阵
