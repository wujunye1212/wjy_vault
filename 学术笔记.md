# 论文笔记

## 一、EEG综述类

### 1.A review of Graph Neural Networks for Electroencephalography data analysis（用于脑电图数据分析的图神经网络综述）Neuro computing 2023

本文章进行了汇总，可以作为引子，有几个方向的介绍，比如：癫痫、情绪、脑机接口、精神疾病。

对于精神疾病来说：引用文献《Classification of first-episode schizophrenia, chronic schizophrenia and healthy control based on brain network of mismatch negativity by graph neural network》展示了

![image-20251117203610318](assets/学术笔记/image-20251117203610318.png)

 "CNN 的成功可归因于其分层结构, 这使它们能够从欧几里得域的数据中提取和整合多尺度特 征。非欧几里得域(例如社交网络、基因数据和脑网络) 中的数据可以通过图进行编码,这些图不仅包含量化元素, 还包含它们之间的关系。"

## 二、EEG无代码论文

### 1.EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model（EEGFormer:迈向可迁移且可解释的大规模脑电图基础模型）

![image-20251118101252869](assets/学术笔记/image-20251118101252869.png)

对大量eeg进行预训练，对每个频段编码

## 三、EEG有代码论文

## 四、计算机视觉有代码论文

## 五、计算机视觉无代码论文



## 图卷积知识



# 小技巧随记

### 1.GCN知识

标准图卷积网络（GCN）层（Kipf & Welling 2017）简化卷积

GCN 层通过对邻接矩阵的归一化，将消息聚合和特征转换合并在一个线性操作中。GCN 层的特征传播公式：$\mathbf{H}^{(l+1)} = \sigma \left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(l)} \mathbf{W}^{(l)} \right)$，其中**$H^{(l)} \in \mathbb{R}^{N \times F_{in}}$**：第 $l$ 层的节点特征矩阵。$N$ 是节点数，$F_{in}$ 是输入特征维度，**$W^{(l)} \in \mathbb{R}^{F_{in} \times F_{out}}$**：第 $l$ 层可学习的权重矩阵（线性变换），**$\tilde{A} = A + I_N$**：添加了自环（Self-loops）的邻接矩阵。为什么要加 $I_N$？如果不加自环，节点在更新时只聚合邻居的信息，而忽略了自身的特征。$\tilde{A}$ 确保了自身特征也参与下一次迭代，**$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$**：$\tilde{A}$ 的度矩阵（Degree Matrix），是一个对角矩阵，**$\sigma(\cdot)$**：非线性激活函数，通常使用 $\text{ReLU}$。

前向传播：

**第一步**：特征变换 首先对节点特征进行线性投影：$Z^{(l)} = H^{(l)} W^{(l)}$维度变换$(N \times F_{in}) \times (F_{in} \times F_{out}) \rightarrow (N \times F_{out})$，物理意义：这一步类似于传统 CNN 中的 $1 \times 1$ 卷积或 MLP，旨在将特征映射到新的高维或低维空间，以提取更高阶的语义特征。 

**第二步**：图结构归一化与消息聚合这是GCN的灵魂所在。我们将变换后的特征 $Z^{(l)}$ 左乘归一化的邻接矩阵：$$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$$，$$H_{agg} = \hat{A} Z^{(l)}$$对称归一化：$\hat{A}_{ij} = \frac{\tilde{A}_{ij}}{\sqrt{\tilde{d}_i \tilde{d}_j}}$。相比于随机游走归一化（$D^{-1}A$），对称归一化保持了矩阵的对称性，这对谱分析至关重要。物理意义：它在聚合邻居信息时，不仅考虑了节点 $i$ 自身的度（度大则权重低，避免hub节点过度支配），也考虑了邻居 $j$ 的度（邻居度大，说明该邻居的影响力被分散了，传给 $i$ 的权重也应降低）。消息传递：这一步实现了局部平滑，每个节点 $i$ 收集其一阶邻居（包括自己）的加权特征和。

第一步：图卷积的原始定义（谱域视角）

​	在图像中，卷积是利用固定大小的核在网格上滑动。但在图上，节点排列无序，无法直接“滑动”。 于是数学家利用**卷积定理**：$$f * g = \mathcal{F}^{-1} \big( \mathcal{F}(f) \cdot \mathcal{F}(g) \big)$$即：时域（空域）的卷积等于频域的乘积。要在图上做卷积，我们需要定义图的“傅里叶变换”。基底：图的拉普拉斯矩阵 $L = D - A$ 的特征向量矩阵 $U$ 就是图的傅里叶基。变换：图信号 $x$ 的傅里叶变换是 $\hat{x} = U^T x$。那么，**图卷积最初被定义为**：$$g_{\theta} \star x = U g_{\theta}(\Lambda) U^T x$$，其中：$x$ 是输入信号（节点特征）。$U$ 是拉普拉斯矩阵的特征向量。$g_{\theta}(\Lambda)$ 是卷积核（滤波器），是我们需要学习的对角矩阵参数。**痛点**：计算特征向量矩阵 $U$ 需要对 $L$ 进行特征分解，复杂度是 $O(N^3)$。对于大图，这根本不可行。

第二步：切比雪夫多项式近似

​	为了避免计算 $U$，学者们引入了**切比雪夫多项式**来近似那个卷积核 $g_{\theta}(\Lambda)$。原理是：任何函数都可以用多项式级数逼近。此时卷积变成了：$$g_{\theta'} \star x \approx \sum_{k=0}^K \theta'_k T_k(\tilde{L}) x$$​，$T_k$ 是 $k$ 阶切比雪夫多项式，$\tilde{L}$ 是归一化后的拉普拉斯矩阵。**K**：代表卷积核的大小，也就是感受野。$K=1$ 代表只看一阶邻居，$K=2$ 代表看二阶邻居。**好处**：不再需要特征分解，只需要算矩阵的幂 $L^k$，这就变成了局部操作，计算量大大降低。

第三步：Kipf & Welling 的“一阶近似” (GCN 2017)

​	Kipf 做了一个非常大胆的简化，把上面的切比雪夫近似推到了极致。1. 限制 K=1（一阶近似）Kipf 认为，我们不需要在单层里看得很远，只看一阶邻居（$K=1$）就够了，通过堆叠多层网络来扩大感受野。当 $K=1$ 时，公式简化为关于 $\tilde{L}$ 的线性函数：$$g_{\theta'} \star x \approx \theta'_0 x + \theta'_1 \tilde{L} x$$，参数共享与约束现在的公式里有两个参数 $\theta'_0$ 和 $\theta'_1$。为了防止过拟合和减少计算量，Kipf 强制令 $\theta'_0 = - \theta'_1 = \theta$。结合拉普拉斯矩阵的定义（$\tilde{L} = I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$），推导过程如下：$$\begin{aligned} g \star x &\approx \theta(I) x - \theta(\tilde{L}) x \\ &= \theta(I - \tilde{L}) x \\ &= \theta \left( I - (I - \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}) \right) x \\ &= \theta \left( \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} \right) x \end{aligned}$$

看！这就出现了 $\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$ 这个核心结构。Renormalization Trick (重归一化技巧)上面推导出的 $\tilde{A}$ 是原始邻接矩阵，对角线为0。如果不处理，直接乘这个矩阵，经过深层网络后，特征值的范围会使得数值不稳定（梯度爆炸或消失）。于是，Kipf 提出了 Renormalization Trick：直接用 $\tilde{A} = A + I$ （加了自环的邻接矩阵）来代替上面的结构。最终公式就诞生了：$$Z = \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} X W$$

### 2.对于ai的公式渲染失败

答：使用这个提示词 ，就可以正常渲染数学公式了

请你以标准 LaTeX 格式输出数学公式，具体要求如下：

1. 使用标准 LaTeX 语法，不要使用任何非标准标记（如 标签）。

2. 如果是行内公式，请用单个美元符号 $ 包裹公式。

3. 如果是独立公式块，请用双美元符号 $$ 包裹公式，并将公式居中显示。

4. 确保公式中的符号、运算符、上下标和括号等符合 LaTeX 语法规范。

5. 公式应清晰、简洁，尽量避免使用模糊或不明确的符号。

6. 根据公式的复杂度，决定使用行内公式还是独立公式块。

7. 如果需要，可以为公式添加简短的注释或说明。

   

### 3.模块缝合问题

对于这个问题

```python
    def forward(self, x):
        x1 = self.in_conv(x)
        x2 = self.down1(x1)
        x2 = self.lsk(x2)
        x3 = self.down2(x2)
        print("x3",x3.shape)
        x3 = to_3d(x3)
        print("x3",x3.shape)
        x3 = self.mv(x3)
        x3 = to_4d(x3,16,16)#这里🔎to_3d要加入两个参数  to_4d(x3,16,16)后面两个16是原始数据的h和w
        # x = x.permute(0, 2, 3, 1)  # 【B, C, H, W】 -> 【B, H, W, C】🔎这个地方尺寸问题 很重要
        # x= x.permute(0, 3, 1, 2)  # 【B, H, W, C】 -> 【B, C, H, W】
        x3 = x3.permute(0, 2, 3, 1)
        x3 = self.md(x3)
        x3 = x3.permute(0, 3, 1, 2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        x = self.out_conv(x)
        return x
# 输入 B C H W,  输出 B C H W
if __name__ == '__main__':
    block = UNet()
    input = torch.rand(3, 1, 64, 64)
    output = block(input)
    print(input.size(), output.size())

```

记得print("符号：",x.shape)，接下来就是bhwc这种尺寸问题：batch：0（有的时候是N），channel：1，height：2，width：3   👉(B/N,C,H,W)

### 4.读文献

方法论：

本质：看论文本身不是目的，而是**手段**，主要看他怎么解决这个问题，用什么思路和角度  

Why-为什么要做这个研究？关注点： **Introduction** 中会阐述**本项研究的重大意义**、要解决什么工程难题或揭示什么物理机制？解决问题的紧迫性等内容。

What-研究发现了什么，**得出了什么结论？**关注点： **摘要中会直接体现最核心的结论**，对于某问题有什么新的认识？提出了什么新的研究方法？得到了什么新的研究结论？先看摘要，由于摘要篇幅受限，更具体的结论可到 Conclusion 或 discussion 中查看。

How-研究是如何实施的，用了什么方法/技术？关注点：这部分属于较为具体、细致的内容，研究的实施过程就是论文的主体内容。对于试验类论文，包括试验目的与思路、试验设备与材料准备、试验方法与步骤、试验现象、试验结果、试验机理揭示等。对于**数值计算类**论文，包括工程背景介绍、数值模型（本构、参数、标定方法）、试验/实测/理论验证、数值计算工况、计算结果分析、工程问题机理揭示等。对于**理论推导类论文**，包括理论简介、理论推导、试验/实测/数值模拟验证、算例分析、参数分析等。

通过这三个问题，推出最终目的科研 Ideas，即这篇文献有哪些不足？ 你可以在哪些方面进行创新？ 可表示为WWH→Ideas。

### 5.创新点

方法论：

找创新点主要在于文献顶刊，发现文章缺陷，在什么地方还有可以改进的方向。**文献的不足之处主要有**：对于 XX 问题少有研究；针对 XX 问题或工况，缺乏有效可行的分析评价方法；仅阐述试验现象，未揭示现象背后的物理机制，未总结出有效的预测方法；缺乏实测验证；理论假设不合理；理论推导不严密；数值计算参数过多且难以标定；计算效率过低；计算误差较大；试验结果不足以支撑得出现有结论等。  

科研创新性排序：**新问题新方法（top期刊成果）>老问题新方法（top、主流期刊成果）>新问题老方法（主流期刊成果）>老问题老方法**

**目前只掌握模块缝合** ；但是模块缝合有简单串并联、内嵌

### 6.打印模型细节

实例化模型

```python
if __name__=='__main__':
	vit = VisionTransformer()
	print(vit)
```

### 7.python基础

__是强制私有       _就是明确私有外部也可以调用

```python
# 定义一个类（蓝图）
class Dog:
    # __init__ 是构造函数，在实例化时自动调用
    # self 代表实例本身
    def __init__(self, name, breed):
        # 给实例添加属性
        self.name = name
        self.breed = breed

    # 给实例添加方法
    def bark(self):
        print(f"{self.name} 汪汪叫！")

# --- 实例化过程 ---
# 根据 Dog 类，创建一个名为 "旺财" 的实例
my_dog = Dog("旺财", "中华田园犬")

# my_dog 就是一个实例，它拥有 name 和 breed 属性，以及 bark 方法
print(my_dog.name)  # 输出: 旺财
print(my_dog.breed) # 输出: 中华田园犬
my_dog.bark()       # 输出: 旺财 汪汪叫！
```

| Layer                                             | Output shape        | 参数项说明                           | 参数量 (个) |
| ------------------------------------------------- | ------------------- | ------------------------------------ | ----------- |
| 输入                                              | `[N, M, 64]`        | —                                    | —           |
| Pointwise Conv1D (1×1)                            | `[N, M, 64]`        | 权重 `1×64×64`, bias `64`            | 4,160       |
| Shared MLP (64→16→64)                             | `[N,1,64]` (logits) | 权重 `64×16 + 16×64`, biases `16+64` | 2,128       |
| LayerNorm (channel)                               | `[N,1,64]`          | γ,β per channel                      | 128         |
| **通道注意力小计**                                |                     |                                      | **6,416**   |
| Expand dim -> `[N,M,64,1]`                        |                     |                                      |             |
| Depthwise convs (5×5,1×7,7×1,1×11,11×1,1×21,21×1) | `[N,M,64,1]`        | depthwise kernels 总计（与 F 无关）  | 103         |
| Spatial 1×1 Conv2D (proj to 1 ch)                 | `[N,M,64,1]`        | weight 1, bias 1                     | 2           |
| LayerNorm (spatial)                               | `[N,M,64,1]`        | γ,β (c=1)                            | 2           |
| Output Conv1D (1×1)                               | `[N,M,64]`          | 权重 `1×64×64`, bias `64`            | 4,160       |
| **空间注意力小计**                                |                     |                                      | **4,267**   |
| **模块总计**                                      | `[N,M,64]`          |                                      | **10,683**  |

<<<<<<< HEAD
### 8.BrainNet Viewer 10-20系统node文件
=======
### 8.BrainNet Viewer 10-20通道
>>>>>>> 2ae204cd0e6f5ea517932888aa768b2ee7e77158

-21.5	66.9	12.1	1	1	Fp1
24.3	66.3	12.5	1	1	Fp2
-39.7	25.3	44.7	2	1	F3
41.9	27.5	43.9	2	1	F4
-49.1	-20.7	53.2	3	1	C3
50.3	-18.8	53.0	3	1	C4
-41.4	-67.8	42.4	5	1	P3
44.2	-65.8	42.7	5	1	P4
-25.8	-93.3	7.7	6	1	O1
25.0	-95.2	6.2	6	1	O2
-52.1	28.6	3.8	2	1	F7
53.2	28.4	-3.1	2	1	F8
-65.8	-17.8	-2.9	4	1	T7
67.4	-18.5	-3.4	4	1	T8
-55.9	-64.8	0.0	4	1	P7
56.4	-64.4	0.1	4	1	P8
0.0	26.8	60.6	2	1	Fz
0.8	-21.9	77.4	3	1	Cz
0.7	-69.3	56.9	5	1	Pz
-62.8	-46.6	-14.66	4	1	TP9
62.8	-46.58	-14.6	4	1	TP10  

这是node文件 edge文件对角线是1 长宽为电极通道数

### 9.Diffusion知识

![image-20251126151607551](assets/学术笔记/image-20251126151607551.png)

$q(x_t|x_{t-1})$ 这个表明他是在前一项的基础上加噪声所以是条件概率（马尔可夫性我们只需要知道它紧邻的上一步 $x_{t-1}$ 就足够了。**我们不需要知道 $x_0, x_1, \dots, x_{t-2}$ 是什么样子的**），选择高斯噪声原因是两个独立高斯分布随机变量相加，结果仍然服从高斯分布，

<img src="assets/学术笔记/image-20251127192513401.png" alt="image-20251127192513401" style="zoom: 67%;" />$z \sim \mathcal{N}(\mu, \sigma^2)$代表定义了一个随机变量z，它服从一个一般正态分布（高斯分布)。这个分布均值（中心位置是$\mu$)，方差（离散程度是$\sigma^2$)，$\frac{z - \mu}{\sigma} \sim \mathcal{N}(0, I)$这是一个标准化过程，如果我们把变量 $z$ 减去它的均值 $\mu$，再除以它的标准差 $\sigma$，得到的新变量就会服从标准正态分布。标准正态分布是最简单的高斯分布，其均值为 0，方差为 1（或者在**多维情况下为单位矩阵 $I$**)**这种变换后的变量常被称为 Z-score。**

$z = \mu + \sigma \cdot \epsilon$    和  $\epsilon \sim \mathcal{N}(0, I)$ 这两行是逆操作，也是重参数化技巧核心，引入服从标准正态分布的辅助噪声变量$\epsilon$

任意一个复杂的正态分布变量 $z$，都可以看作是由一个确定的均值部分 $\mu$，加上一个被标准差 $\sigma$ 缩放过的标准噪声 $\epsilon$ 组合而成的。

**$x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_{t-1}$** 这里的均值 $\mu$ 是 $\sqrt{\alpha_t}x_{t-1}$，这里的方差 $\sigma^2$ 是 $1-\alpha_t$，所以标准差 $\sigma$ 是 $\sqrt{1-\alpha_t}$

### 10.深度学习路线

李沐--吴恩达---李宏毅--爆肝杰哥（入门）--邱锡鹏--小土堆（入门）--shuhuai008（硬核手推）--

11.
