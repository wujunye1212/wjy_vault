
# 1.每日规划








































# 2.科研团队追踪




# 2.git hub ssh登录
换新项目也要在本地生成公钥🔑👉上传到github

# 3.答辩


# 4.论文总结模板


# 5.工作流
**googgle scholar+web of science+connected papers→zotero+*obsidian→（多端同步github）→learning

# 6.开题
毕业时间➖一年半=开题时间
开题👉中期👉盲审👉预答辩👉答辩
**开题到盲审时间是1年**
**中期到盲审时间是4个月**

# 7.燕大微后勤搜索打电话防止邮箱被注销

# 8. 博士答辩
### 1. 解决科学问题一：如何解决脑电信号低信噪比与个体差异大的特征难点？

**图一的问题：** EEG 信号本身噪声大（眨眼、肌肉电等干扰），且不同人的大脑信号模式差异巨大，导致模型很难学到通用的特征。

**图二的解决方案：** **`Dynamic-Static EEG-fusion Encoder` (动静结合的神经聚合器)**

- **多视图融合 (Dynamic + Static):**
    
    - 架构图中左侧将输入分为了 **Static EEG ($e_s$)** 和 **Dynamic EEG ($e_d$)**。
        
    - **逻辑：** 静态特征捕捉个体的基准状态（Baseline），动态特征捕捉对视觉刺激的瞬时反应。通过融合这两者，模型可以“减去”个体的背景噪声，提取出真正与视觉相关的共性信号。
        
- **Neural Aggregator (Cross Attention):**
    
    - 图中核心的蓝色方块使用了 **Cross Attention (Q, K, V)** 机制。
        
    - **逻辑：** Attention 机制本质上是一个**滤波器**。它让模型学会“注意力集中”，自动忽略掉噪音部分（低权重），只关注那些对生成 3D 物体有用的高价值信号片段。这极大地提高了有效信号的利用率（即提高了信噪比）。
        

### 2. 解决科学问题二：如何构建高保真的脑-图语义生成模型？

**图一的问题：** 仅仅解码出形状是不够的，如何让解码出来的东西在**语义上**是对的？（比如看了“红色的消防栓”，不能生成“红色的杯子”）。

**图二的解决方案：** **`CLIP Image Encoder` 对齐与 `Semantically-guided` 分支**

- **语义对齐 (Contrastive & MSE):**
    
    - 架构图下方有一个 **CLIP Image Encoder (Frozen)**。生成的 EEG 特征 ($f_v$) 被强制要求与 CLIP 的图像特征对齐（通过 Contrastive Loss 和 MSE Loss）。
        
    - **逻辑：** CLIP 已经在大规模数据上学到了完美的语义空间。通过强制 EEG 信号向 CLIP 空间靠拢，你实际上是在利用 CLIP 强大的知识库来“纠正”脑电信号的语义偏差。这确保了大脑想的是“椅子”，生成的特征向量就是“椅子”的向量。
        
- **双流解耦 (Linear Projections):**
    
    - Decoder 前端将特征分流为 **Geometry feature ($f_g$)** 和 **Appearance feature ($f_a$)**。
        
    - **逻辑：** 这种解耦确保了模型不仅能学到“它是个物体”，还能精准控制“它长什么样（几何）”和“它是什么颜色（外观）”，从而实现高保真。
        

### 3. 解决科学问题三：如何突破二维限制，实现三维(3D)视觉感知的解码？

**图一的问题：** 现有的 SOTA 方法（如 NeuroIPS 2022, CVPR 2023）大多只能生成 2D 图片。人类的视觉感知本质是 3D 的，现有的方法丢失了深度和空间结构信息。

**图二的解决方案：** **`Colored Point Cloud Decoder` (基于扩散的点云生成器)**

- **从像素(Pixel)到点云(Point Cloud):**
    
    - 架构图右侧明确是一个 **Point Cloud Decoder**，输出的是三维点云而非二维图像。
        
    - **逻辑：** 这是最直接的突破。你不再预测 RGB 矩阵，而是预测 XYZ 坐标 + RGB 颜色。
        
- **Denoising PVN (Point Voxel Network) / Diffusion:**
    
    - 图中显示了一个循环箭头 $\times T$ 和 `Denoising PVN`。
        
    - **逻辑：** 这是一个**3D 扩散模型 (Diffusion Model)**。扩散模型在生成任务上具有极强的创造力。它从一个随机的 3D 噪声球开始，根据前面提取的脑电语义特征 ($f_g, f_a$)，一步步“雕刻”出物体的 3D 形状。
        
- **Shape & Color Generation 分步生成:**
    
    - 图中先经过 `Shape Generation`，再进入 `Color Generation`。
        
    - **逻辑：** 3D 重建比 2D 生成难得多。你的模型采用了“先搭骨架（形状），再上色（颜色）”的策略（Coarse-to-Fine），这比直接生成彩色 3D 物体更稳定，从而成功实现了从 2D 解码到 3D 解码的跨越。
        

---

### 总结：你的 Storyline (故事线)

如果你在做 Presentation，你可以这样串联：

> “针对信噪比低的问题，我们设计了动静结合的注意力聚合模块（图二左上）；
> 
> 针对语义对齐难的问题，我们引入了CLIP 监督的语义特征对齐策略（图二左下）；
> 
> 针对缺乏 3D 感知的问题，我们提出了基于扩散模型的点云生成解码器（图二右侧）。
> 
> 这是一个端到端的、从脑电信号直接重建彩色 3D 点云的全新框架。”
